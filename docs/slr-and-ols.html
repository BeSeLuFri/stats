<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Simple Linear Regression and OLS</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Econometrics with R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Introduction</a>
</li>
<li>
  <a href="objectsndata.html">Objects and Data</a>
</li>
<li>
  <a href="datamanagement.html">Data Management</a>
</li>
<li>
  <a href="linreg.html">Linear Regression</a>
</li>
<li>
  <a href="dataviz.html">DataViz</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Statistical Basics and Inference
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="random-variables.html">Random Variables and Distributions</a>
    </li>
    <li>
      <a href="features_of_pdf.html">Features of Random Variables</a>
    </li>
    <li>
      <a href="estimators_and_bias.html">Inference 1: Estimation</a>
    </li>
    <li>
      <a href="hypothesis_tests.html">Inference 2: Hypothesis Tests</a>
    </li>
    <li>
      <a href="slr-and-ols.html">Simple Linear Regression and OLS</a>
    </li>
  </ul>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Simple Linear Regression and OLS</h1>

</div>


<hr />
<div id="contents-and-goals" class="section level1">
<h1><span class="header-section-number">1</span> Contents and Goals</h1>
<ul>
<li>Get to know the Simple Linear Regression Model (SLM)
<ul>
<li>Economic and Statistical interpretation</li>
<li>Ordinary Least Squares</li>
</ul></li>
</ul>
<hr />
</div>
<div id="simple-linear-regression-and-ols" class="section level1">
<h1><span class="header-section-number">2</span> Simple Linear Regression and OLS</h1>
<p>Now that we have learned about correlation and testing, we want to predict one variable from another. One simple way to do so is using linear regression. Simple linear regression (SLR) is about predicting a dependent variable (<em>regressand</em>) from one independent variable (<em>regressor</em>). A SLR model is given by: <span class="math display">\[y = \beta_0 + \beta_1x + \epsilon\]</span> where <span class="math inline">\(y\)</span> is the regressand and <span class="math inline">\(x\)</span> the regressor. <span class="math inline">\(\epsilon\)</span> denotes the error of the model, i.e. the residuals. Regression is often used to model real-world relationships.</p>
<p>Basically, a linear regression model is nothing else other than a linear function <span class="math inline">\(y=mx+b\)</span>. In the regression case, <span class="math inline">\(\beta_0\)</span> is <span class="math inline">\(b\)</span>, and <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(m\)</span>, the slope. The regression line is the one line that crosses or passes the observation with the smallest amount of squared error.</p>
<pre class="r"><code>library(ggplot2)
student_id = c(1,2,3,4,5,6,7,8,9,10)
x = c(95,85,80,73,70,65,60,58,55,51)
y = c(85,95,75,70,65,70,62,53,52,51)
grades_df = data.frame(student_id, x, y)
ggplot(grades_df, aes(x, y)) + 
  geom_point() + 
  labs(x=&quot;Statistics Grade&quot;, y=&quot;Econometrics Grade&quot;) +
  geom_smooth(method=&quot;lm&quot;, colour=&quot;red&quot;, se=T, alpha=0.1) +
  geom_text(aes(label=student_id),hjust=-1, vjust=1, color=&quot;gray&quot;) +
  theme_minimal()</code></pre>
<p><img src="slr-and-ols_files/figure-html/unnamed-chunk-1-1.png" width="864" /> Here, we have a linear model that predicts a student’s Econometrics grade from his or her Statistics grade. Notice that the line does not cross a single data point, but it minimises the squared deviations of all the single points from the line. Those deviations are called “residuals” and in a linear regression model, they sum up to zero. Lets take a look at the residuals:</p>
<pre class="r"><code>resid = summary(lm(y~x, data=grades_df))$residuals; resid</code></pre>
<pre><code>##          1          2          3          4          5          6 
## -7.0271078 12.3632441 -2.9415800 -1.3683337 -3.5512281  6.1439478 
##          7          8          9         10 
##  2.8391237 -4.2828059 -2.4657004  0.2904404</code></pre>
<pre class="r"><code>round(sum(resid))</code></pre>
<pre><code>## [1] 0</code></pre>
<p>Look at the graph. The rightmost point (student 1) deviates by <span class="math inline">\(-7.03\)</span> (the first entry of our residuals), the second by <span class="math inline">\(12.36\)</span> etc.</p>
<p>Usually, researchers are interested in the amount of change in <span class="math inline">\(y\)</span> that occur when <span class="math inline">\(x\)</span> changes. As in any linear function, the rate of change is given by <span class="math inline">\(\beta_1\)</span>. More formally: <span class="math display">\[\frac{\delta y}{\delta x} = \beta_1\]</span></p>
<div id="interpretation-by-example" class="section level2">
<h2><span class="header-section-number">2.1</span> Interpretation by example</h2>
<p>Lets look again at the example. We have ten students who took exams in Statistics and exams in Econometrics. We know what grades were in percentage points. Now, we would like to predict the Econometrics grade of any other student who took the Statistics exam. Our model looks as follows:</p>
<p><span class="math display">\[\hat{econgrade} = \hat{\beta_0} + \hat{\beta_1}statsgrade + \hat{\epsilon}\]</span> If you wonder about the hats (<span class="math inline">\(\hat{}\)</span>), we use them because now that we look at empirical data, we cannot make any claims about the true or theoretical distribution, but only about our empirical data. We only have estimators, so we use hats to mark them accordingly.</p>
<p><img src="slr-and-ols_files/figure-html/unnamed-chunk-3-1.png" width="960" /></p>
<pre class="r"><code>summary(lm(y~x, data=grades_df))$coefficients</code></pre>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 2.8187652 10.0801739 0.2796346 0.7868516450
## x           0.9390352  0.1429934 6.5669819 0.0001753288</code></pre>
<p>Once again, we see a clear linear relationship between the students’ statistics grades and the econometrics grades. By how much does a student’s Econometrics grade increase if the student’s Statistics grade was better by one?</p>
<p>Look at the Estimate column. The first row in that column (2.82) is the <em>intercept</em> of the line with the y-axis. The interpretation of that value (which is not always meaningful) would be that if a student scored zero points in his statistics exam, he will score 2.82 on the Econometrics exam. The second row in the Estimate column is the estimate of the beta coefficient for our x, which is the Statistics grade. It tells us that the <span class="math inline">\(\hat{\beta_1}\)</span> is 0.94. We would interpret this value as follows: “Ceteris paribus (with all else being equal), an increase of 1 of the Statistics grade implies an increase of 0.94 of the Econometrics grade.”. Many researchers are also interested in the statistical significance of our estimate. We find the <span class="math inline">\(p\)</span>-value to be way below common standards (0.0002), so we would reject our null hypothesis (which was that the estimate equal to zero) and say that our beta coefficient is statistically significant. What does this mean? It basically means that we can believe that a better Statistics grade really implies a better Econometrics grade.</p>
<p>But we have an essential problem with this interpretation, because we do not actually know that we have a causal relationship here or just a correlation. And right now, we do not know whether the line we drew represents the best guess for a prediction. To answer these questions, we need to know more.</p>
</div>
<div id="ordinary-least-squares" class="section level2">
<h2><span class="header-section-number">2.2</span> Ordinary least squares</h2>
<p>As we have heard before, the regression line is a line that fits our points so that it minimises the squared deviations (residuals) of the points from the line. To construct such a line, we need to know the intercept and the slope coefficient(s) for <span class="math inline">\(x\)</span>. The formulas are quite simple: <span class="math display">\[\begin{align}\hat{\beta_1}&amp;=\frac{Cov[x,y]}{Var[x]}\\\hat{\beta_0}&amp;=\bar{y} - \hat{\beta_1}\bar{x}\text{.}\end{align}\]</span> The proof is fun because it is simple. We set up a formal minimization problem because we want to minimize <span class="math inline">\(\hat{\epsilon}^2\)</span>. <span class="math display">\[\min\limits_{\hat{\beta_0},\hat{\beta_1}}\sum_{i=1}^n\hat{u}_i^2 = \sum_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_1)^2\]</span> We minimise the function by taking the first order conditions (FOCs) by <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>. The first FOCs is (as derived by <span class="math inline">\(\hat{\beta_0}\)</span>: <span class="math display">\[\begin{aligned}&amp;2\times\sum_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i) \stackrel{!}{=} 0 \text{ | :2} \\
\iff &amp; \sum_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_i) \stackrel{!}{=} 0\\
\iff &amp; \sum_{i=1}^n(y_i)-\sum_{i=1}^n(\hat{\beta}_0)-\sum_{i=1}^n(\hat{\beta}_1x_i)\stackrel{!}{=}0\\
\iff &amp; \sum_{i=1}^n(y_i)-n\times\hat{\beta}_0-\sum_{i=1}^n(\hat{\beta}_1x_i)\stackrel{!}{=}0\text{ | } :n\\
\iff &amp; \bar{y}-\hat{\beta_0}-\hat{\beta_1}\bar{x}\stackrel{!}{=}0\\
\iff &amp; \hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}
\end{aligned}\]</span> and then we derive by <span class="math inline">\(\hat{\beta_1}\)</span>: <span class="math display">\[\begin{aligned}&amp;2\times\sum_{i=1}^n-x_i(y_i-\hat{\beta}_0-\hat{\beta}_1x_i) \stackrel{!}{=} 0\text{ | :(-2)} \\
\iff &amp; \sum_{i=1}^nx_i(y_i-\hat{\beta}_0-\hat{\beta}_1x_i) \stackrel{!}{=} 0\\
\iff &amp; \sum_{i=1}^nx_i(y_i-(\bar{y}-\hat{\beta_1}\bar{x})-\hat{\beta}_1x_i) \stackrel{!}{=} 0\\
\iff &amp; \sum_{i=1}^nx_i(y_i-\bar{y})+\sum_{i=1}^nx_i(\hat{\beta_1}\bar{x}-\hat{\beta_1}x_i) \stackrel{!}{=} 0\\
\iff &amp; \sum_{i=1}^nx_i(y_i-\bar{y})-\hat{\beta_1}\sum_{i=1}^nx_i(x_i-\bar{x}) \stackrel{!}{=} 0\\
\iff &amp; \sum_{i=1}^nx_i(y_i-\bar{y})=\hat{\beta_1}\sum_{i=1}^nx_i(x_i-\bar{x}) \text{ | *}\\
\iff &amp; \sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})=\hat{\beta_1}\sum_{i=1}^n(x_i-\bar{x})^2\\
\iff &amp; \hat{\beta_1} = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} = \frac{Cov[x,y]}{Var[x]}
\end{aligned}\]</span></p>
<p>The transformation denoted by the * results from the fact that <span class="math inline">\(\sum_{i=1}^nx_i(x_i-\bar{x}) = (x_i-\bar{x})^2\)</span> and <span class="math inline">\(\sum_{i=1}^nx_i(y_i-\bar{y}) = (x_i-\bar{x})(y_i-\bar{y})\)</span></p>
</div>
<div id="law-of-iterated-expectations" class="section level2">
<h2><span class="header-section-number">2.3</span> Law of Iterated Expectations</h2>
<p>We will shortly learn under which conditions we can assume that we’re looking at a causal relationship of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span>. But before we do that, we need to know and understand the Law of Iterated Expectations (LEI). It states that the Expectation of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> is equal to the expectation of <span class="math inline">\(y\)</span>: <span class="math display">\[\mathbb{E}\left[Y\right] = \mathbb{E}\left[\mathbb{E}\left[Y|X\right]\right]\]</span> To get an idea why this holds, consider a game of darts being played in another room and you want to guess the landing spot of a dart. Let <span class="math inline">\(Y\)</span> denote this spot. Without any prior information, our best guess for the value <span class="math inline">\(Y\)</span> takes on would be the center <span class="math inline">\(c\)</span> of the board: <span class="math display">\[\mathbb{E}\left[Y\right] = c\]</span> Now suppose that every time a dart is thrown, someone watching in the other room tells us whether the dart landed in the upper or lower half of the board. Let this be another random variable <span class="math inline">\(X\)</span>. By the information of <span class="math inline">\(X\)</span> we can improve our guess for the landing spot - if <span class="math inline">\(X = upper\)</span>, then our best guess for <span class="math inline">\(Y\)</span> might be the center of the upper half of the board, <span class="math inline">\(\mathbb{E}\left[Y|X = \text{upper}\right] = c^{\text{upper}}\)</span>, if <span class="math inline">\(X = lower\)</span>, then our best guess for <span class="math inline">\(Y\)</span> would be the center of the lower half of the board, <span class="math inline">\(\mathbb{E}\left[Y|X = \text{lower}\right] = c^{\text{lower}}\)</span> Thus, the expectation of <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(X\)</span> depends on whether <span class="math inline">\(X = up\)</span> or <span class="math inline">\(X = low\)</span>. But since <span class="math inline">\(X\)</span> is a random variable, <span class="math inline">\(\mathbb{E}\left[Y|X\right]\)</span> is also a random variable. When iterating Expectations like <span class="math inline">\(\mathbb{E}\left[\mathbb{E}\left[Y|X\right]\right]\)</span> we take expectations over the random variable <span class="math inline">\(\mathbb{E}\left[Y|X\right]\)</span>, i.e., averaging the two best guesses of <span class="math inline">\(X\)</span> (upper and lower). When averaging those guesses proportional to how likely each <span class="math inline">\(X\)</span> outcome will be, then we expect that the averaged guess <span class="math inline">\(\mathbb{E}\left[\mathbb{E}\left[Y|X\right]\right]\)</span> agrees with the overall “direct” best guess <span class="math inline">\(\mathbb{E}\left[Y\right]\)</span> Credits to Jimmy Jin for the great intuition behind the LEI.</p>
</div>
<div id="causal-interpretation" class="section level2">
<h2><span class="header-section-number">2.4</span> Causal interpretation</h2>
<p>For a regression to be more than a simple correlation, one has to make assumptions how <span class="math inline">\(x\)</span> is related to the error term <span class="math inline">\(\epsilon\)</span>. For causal interpretations, <span class="math inline">\(x\)</span> must not include information about <span class="math inline">\(\epsilon\)</span>, that is, they must not be correlated. If they would be correlated, it is still possible that something else than <span class="math inline">\(x\)</span> predicts <span class="math inline">\(y\)</span>. Mathematically speaking: <span class="math display">\[\mathbb{E} \left[\epsilon|x\right] = 0\]</span> This is called the <strong>zero conditional mean assumption</strong>. If it holds, we have <strong>exogeneity</strong>. In the example with the statistics and econometrics grade, we obviously do not have exogeneity, as the statistics grade is for example also highly correlated with the number of hours spend studying, that may also affect the econometrics grade. In this case, <span class="math inline">\(\mathbb{E}\left[\epsilon|statsgrade\right] \neq 0\)</span> Following from this and the Law of Iterated Expectations: <span class="math display">\[\begin{aligned}\mathbb{E} \left[\epsilon|x\right] &amp;= \mathbb{E}\left[\epsilon\right] = 0\text{,}\end{aligned}\]</span> therefore: <span class="math display">\[\begin{aligned}Cov\left[x, \epsilon\right] &amp;= \mathbb{E}\left[x\epsilon\right] - \mathbb{E}\left[X\right] \mathbb{E}\left[\epsilon\right]  = 0\end{aligned}\]</span> Effectively, if the expected value of the error term is zero, the model is without error and thus average value of <span class="math inline">\(y\)</span> can be expressed as a linear function of <span class="math inline">\(x\)</span>: <span class="math display">\[\begin{aligned}\mathbb{E}\left[y|x\right] &amp;= \mathbb{E}\left[\beta_0 + \beta_1x + \epsilon|x\right] \\ &amp;= \beta_0 + \beta_1x + \mathbb{E}\left[\epsilon|x\right] \\ &amp;= \beta_0 + \beta_1x \end{aligned}\]</span></p>
</div>
<div id="assumptions" class="section level2">
<h2><span class="header-section-number">2.5</span> Assumptions</h2>
<p>Lastly, before interpreting SLR models, one should be aware of its assumptions, namely:</p>
<ol style="list-style-type: decimal">
<li>The population model is linear in its parameters: <span class="math display">\[y = \beta_0 + \beta_1x + \epsilon\]</span></li>
<li>The sample at hand is a random sample from the population model.<br />
</li>
<li>There is variation in the <span class="math inline">\(x_i\)</span>: <span class="math display">\[\sum_{i=1}^n (x_i - \bar{x})^2 &gt; 0\]</span></li>
<li><span class="math inline">\(\mathbb{E} \left[\epsilon|x\right] = 0\)</span> and therefore <span class="math inline">\(\mathbb{E} \left[\epsilon_i|x_i\right] = 0\)</span></li>
<li>Homoskedasticity (Equality of variances) is given: <span class="math display">\[Var\left[\epsilon|x\right] = Var\left[\epsilon_i|x_i\right] = \sigma^2\]</span></li>
</ol>
<p>Note that this only holds as long as the the assumption of independence is met, i.e. <span class="math inline">\(\frac{\delta \epsilon}{\delta x} = 0\)</span></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
